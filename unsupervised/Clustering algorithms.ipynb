{"cells":[{"cell_type":"markdown","metadata":{"id":"LP36auNXTHHd"},"source":["Heirarchial clustering\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NjjRd2S1TBNm"},"outputs":[],"source":["# Hierarchical Clustering\n","\n","# Importing the libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Importing the dataset\n","dataset = pd.read_csv('Data.csv')\n","X = dataset.iloc[:, [3, 4]].values\n","\n","# Using the dendrogram to find the optimal number of clusters\n","import scipy.cluster.hierarchy as sch\n","dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n","plt.title('Dendrogram')\n","plt.xlabel('Customers')\n","plt.ylabel('Euclidean distances')\n","plt.show()\n","\n","# Training the Hierarchical Clustering model on the dataset\n","from sklearn.cluster import AgglomerativeClustering\n","hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\n","y_hc = hc.fit_predict(X)\n","\n","# Visualising the clusters\n","plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n","plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n","plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n","plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n","plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n","plt.title('Clusters of customers')\n","plt.xlabel('Annual Income')\n","plt.ylabel('Spending Score (1-100)')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["\n","k-means clustering"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# K-Means Clustering\n","\n","# Importing the libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Importing the dataset\n","dataset = pd.read_csv('Data.csv')\n","X = dataset.iloc[:, [3, 4]].values\n","\n","# Using the elbow method to find the optimal number of clusters\n","from sklearn.cluster import KMeans\n","wcss = []\n","for i in range(1, 11):\n","    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 0)\n","    kmeans.fit(X)\n","    wcss.append(kmeans.inertia_)\n","plt.plot(range(1, 11), wcss)\n","plt.xlabel('Number of clusters')\n","plt.ylabel('WCSS')\n","plt.show()\n","\n","# Training the K-Means model on the dataset\n","kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 0)\n","y_kmeans = kmeans.fit_predict(X)\n","\n","# Visualising the clusters\n","plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'green', label = 'Cluster 1')\n","plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n","plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'red', label = 'Cluster 3')\n","plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n","plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n","plt.title('Clusters of customers')\n","plt.xlabel('Annual Income (k$)')\n","plt.ylabel('Spending Score (1-100)')\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPftlEtqQ9QC7HfVovIi483","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
